# -*- coding: utf-8 -*-
"""EECS_545_Final_Prox_BCD.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hwe1cdYa63GzVf8HzVzJmjI7GVShoMMu
"""

# Proximal Block Coordinate Descent

# import all relevant libraries
import numpy as np
import torch
from torch import nn
from torch.utils.data import DataLoader
from torchvision import datasets
from torchvision.transforms import ToTensor, Compose, Normalize
import matplotlib.pyplot as plt
import time

# Get cpu or gpu device for training.
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using {device} device")

# set seed to get reproducible results
seed = 10
if torch.cuda.is_available(): torch.manual_seed(seed)

# Download training data from open datasets.
# MNIST 
# Gloabal Mean 0.1307 and Std 0.3081
# MNIST Fashion (Not Sure) but use
# Global Mean 0.5 Std 0.5
# CIFAR 
# No Clue

#### Comment out one to get one dataset
# MNIST
training_data = datasets.MNIST(
    root="data",
    train=True,
    download=True,
    transform=Compose([ToTensor(), Normalize((0.1307,),(0.3081,))]),
)
test_data = datasets.MNIST(
    root="data",
    train=False,
    download=True,
    transform = Compose([ToTensor(), Normalize((0.1307,),(0.3081,))]),
)


# # MNIST Fashion 
# training_data = datasets.FashionMNIST(
#     root="data",
#     train=True,
#     download=True,
#     transform=Compose([ToTensor(), Normalize((0.5,),(0.5,))]),
# )
# test_data = datasets.FashionMNIST(
#     root="data",
#     train=False,
#     download=True,
#     transform = Compose([ToTensor(), Normalize((0.5,),(0.5,))]),
# )

# Manipulate train set
num_labels = 10
channels,height,width = training_data[0][0].size()
total_dim = channels*height*width
N_train = len(training_data)
x_train = torch.empty((N_train,total_dim), device=device)
y_train = torch.empty(N_train, dtype=torch.long)
for i in range(N_train): 
     x_train[i] = training_data[i][0].reshape(1, total_dim)
     y_train[i] = training_data[i][1]
print(y_train.shape)
x_train = torch.t(x_train)
y_train_one_hot = torch.nn.functional.one_hot(y_train, num_classes=-1).t().to(device = device)
y_train = y_train.to(device=device)

# Manipulate test set
N_test = len(test_data)
x_test = torch.empty((N_test,total_dim), device=device)
y_test = torch.empty(N_test, dtype=torch.long)
for i in range(N_test): 
     x_test[i] = test_data[i][0].reshape(1, total_dim)
     y_test[i] = test_data[i][1]
x_test = torch.t(x_test)
y_test_one_hot = torch.nn.functional.one_hot(y_test, num_classes=-1).t().to(device = device)
y_test = y_test.to(device=device)

# define layers of the Neural Network
# Layers: input + 3 hidden + output
d0 = total_dim
d1 = total_dim
d2 = total_dim
d3 = total_dim
d4 = num_labels 

# weights of each layer - random initialization
theta_layer_1 = 0.01*torch.randn(d1, d0, device=device)
theta_layer_2 = 0.01*torch.randn(d2, d1, device=device)
theta_layer_3 = 0.01*torch.randn(d3, d2, device=device)
theta_layer_4 = 0.01*torch.randn(d4, d3, device=device)

# for ease
relu = nn.ReLU()

# forward propagation to initialize a's and Z's
Z1 = torch.mm(theta_layer_1, x_train)
a1 = relu(Z1)
Z2 = torch.mm(theta_layer_2, a1)
a2 = relu(Z2)
Z3 = torch.mm(theta_layer_3, a2)
a3 = relu(Z3) 
Z4 = torch.mm(theta_layer_4, a3)
a4 = Z4

# prameters of proximal block coordinate descent
gamma = 1
rho = 1
alpha = 5

# 
num_iter = 100
loss_squared = np.empty(num_iter)
loss_total = np.empty(num_iter)
train_accuracy = np.empty(num_iter)
test_accuracy = np.empty(num_iter)

def update_a(Z1,Z2,theta): 
    _, d = theta.size()
    I = torch.eye(d, device=device)
    Z1 = relu(Z1)
    a_star = torch.mm(torch.inverse(rho*(torch.mm(torch.t(theta),theta))+gamma*I), rho*torch.mm(torch.t(theta),Z2)+gamma*Z1)
    return a_star

def update_theta(Z, a, theta): 
    d,N = a.size()
    I = torch.eye(d, device=device)
    theta_star = torch.mm(alpha*theta+rho*torch.mm(Z,torch.t(a)),torch.inverse(alpha*I+rho*(torch.mm(a,torch.t(a)))))
    return theta_star

def prox_relu(a, b, gamma, d, N):
    val = torch.empty(d,N, device=device)
    x = (a+gamma*b)/(1+gamma)
    y = torch.min(b,torch.zeros(d,N, device=device))

    val = torch.where(a+gamma*b < 0, y, torch.zeros(d,N, device=device))
    val = torch.where(((a+gamma*b >= 0) & (b >=0)) | ((a*(gamma-np.sqrt(gamma*(gamma+1))) <= gamma*b) & (b < 0)), x, val)
    val = torch.where((-a <= gamma*b) & (gamma*b <= a*(gamma-np.sqrt(gamma*(gamma+1)))), b, val)
    return val

# Run over several epochs
# print('Train on', N_train, 'samples, validate on', N_test, 'samples')
for k in range(num_iter):

    ## Do block wise updates of all layers
    # output layer updates
    a4 = (y_train_one_hot + gamma*Z4 + alpha*a4)/(1 + gamma + alpha)
    Z4 = (gamma*a4 + rho*(torch.mm(theta_layer_4,a3)))/(gamma + rho)
    theta_layer_4= update_theta(Z4,a3,theta_layer_4)
    
    # layer 3 updates
    a3 = update_a(Z3,Z4,theta_layer_4)
    Z3 = prox_relu(a3,(rho*torch.mm(theta_layer_3, a2) + alpha*Z3)/(rho + alpha),(rho + alpha)/gamma,d3,N_train)
    theta_layer_3 = update_theta(Z3,a2,theta_layer_3)
    
    # layer 2 updates
    a2 = update_a(Z2,Z3,theta_layer_3)
    Z2 = prox_relu(a2,(rho*torch.mm(theta_layer_2, a1) + alpha*Z2)/(rho + alpha),(rho + alpha)/gamma,d2,N_train)
    theta_layer_2 = update_theta(Z2,a1,theta_layer_2)

    # layer 1 updates
    a1 = update_a(Z1,Z2,theta_layer_2)
    Z1 = prox_relu(a1,(rho*torch.mm(theta_layer_1, x_train) + alpha*Z1)/(rho + alpha),(rho + alpha)/gamma,d1,N_train)
    theta_layer_1 = update_theta(Z1,x_train,theta_layer_1)

    # compute train statistics of network
    a1_train = relu(torch.mm(theta_layer_1, x_train))
    a2_train = relu(torch.mm(theta_layer_2, a1_train))
    a3_train = relu(torch.mm(theta_layer_3, a2_train))
    prediction = torch.argmax(torch.mm(theta_layer_4, a3_train), dim=0)

    # compute test statistics of network
    a1_test = relu(torch.mm(theta_layer_1, x_test))
    a2_test = relu(torch.mm(theta_layer_2, a1_test))
    a3_test = relu(torch.mm(theta_layer_3, a2_test))
    prediction_test = torch.argmax(torch.mm(theta_layer_4, a3_test), dim=0)
    
    # loss of
    loss_squared[k] = gamma/2*torch.pow(torch.dist(a4,y_train_one_hot,2),2).cpu().numpy()
    loss_total[k] = loss_squared[k] + rho/2*torch.pow(torch.dist(torch.mm(theta_layer_1, x_train),Z1,2),2).cpu().numpy() \
    +rho/2*torch.pow(torch.dist(torch.mm(theta_layer_2, a1),Z2,2),2).cpu().numpy() \
    +rho/2*torch.pow(torch.dist(torch.mm(theta_layer_3, a2),Z3,2),2).cpu().numpy() \
    +rho/2*torch.pow(torch.dist(torch.mm(theta_layer_4, a3),Z4,2),2).cpu().numpy()
    
    # compute training accuracy
    correct_train = prediction == y_train
    train_accuracy[k] = np.mean(correct_train.cpu().numpy())
    
    # compute validation accuracy
    correct_test = prediction_test == y_test
    test_accuracy[k] = np.mean(correct_test.cpu().numpy())
    
    
    print("Epoch: {}/{}..".format(k+1, num_iter),
        "Squared loss: {:.3f}..".format(loss_squared[k]/N_train),
        "Total loss: {:.3f}..".format(loss_total[k]),
        "Train Accuracy: {:.3f}".format(train_accuracy[k]),
        "Test Accuracy: {:.3f}".format(test_accuracy[k]))

plt.rcParams.update({'font.size': 16})
plt.figure(dpi=100)
plt.plot(np.arange(num_iter),  loss_squared/N_train, c='b', linewidth=4)
plt.ylabel("Train Squared Loss")
plt.xlabel("Epoch")
plt.tight_layout()
plt.savefig("/content/trainLoss.png")
plt.show()

plt.figure(dpi=100)
line1, = plt.plot(np.arange(num_iter), train_accuracy, c='b', linewidth=4)
line2, = plt.plot(np.arange(num_iter), test_accuracy, c='r', linewidth=4)
plt.ylabel("Accuracy")
plt.xlabel("Epoch")
plt.legend([line1, line2], ['train', 'test'], loc='lower right')
plt.tight_layout()
plt.savefig("/content/trainTestAcc.png")
plt.show()