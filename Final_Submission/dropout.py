# -*- coding: utf-8 -*-
"""EECS_545_Final_Project_Dropout.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fLDTtnYSN0Mdf9aK3wTXJuoEaJ9LVl_F
"""

# Dropout with Stochastic Gradient Descent

# import all relevant libraries
import numpy as np
import torch
from torch import nn, optim
import torch.nn.functional as F
from torch.utils.data import DataLoader
from torchvision import datasets
from torchvision.transforms import ToTensor, Compose, Normalize
import matplotlib.pyplot as plt
from matplotlib.pyplot import figure

# Get cpu or gpu device for training.
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using {device} device")

# set seed to get reproducible results
seed = 10
if torch.cuda.is_available(): torch.manual_seed(seed)

#### Comment out one to get one dataset
# MNIST
# training_data = datasets.MNIST(
#     root="data",
#     train=True,
#     download=True,
#     transform=Compose([ToTensor(), Normalize((0.1307,),(0.3081,))]),
# )
# trainloader = DataLoader(training_data, batch_size=60000)

# test_data = datasets.MNIST(
#     root="data",
#     train=False,
#     download=True,
#     transform = Compose([ToTensor(), Normalize((0.1307,),(0.3081,))]),
# )
# testloader = DataLoader(test_data, batch_size=10000)


# MNIST Fashion 
training_data = datasets.FashionMNIST(
    root="data",
    train=True,
    download=True,
    transform=Compose([ToTensor(), Normalize((0.5,),(0.5,))]),
)
trainloader = DataLoader(training_data, batch_size=60000)
test_data = datasets.FashionMNIST(
    root="data",
    train=False,
    download=True,
    transform = Compose([ToTensor(), Normalize((0.5,),(0.5,))]),
)
testloader = DataLoader(test_data, batch_size=10000)

# Define the network architecture
# Uniform COnvention
num_labels = 10
channels,height,width = training_data[0][0].size()
total_dim = channels*height*width
d0 = total_dim
d1 = total_dim
d2 = total_dim
d3 = total_dim
d4 = num_labels

model = nn.Sequential(nn.Linear(total_dim,d1),
                      nn.ReLU(),
                      # nn.Dropout(0.2),
                      nn.Linear(d1, d2),
                      nn.ReLU(),
                      nn.Dropout(0.2),
                      nn.Linear(d2, d3),
                      nn.ReLU(),
                      nn.Dropout(0.2),
                      nn.Linear(d3, num_labels),
                      nn.LogSoftmax(dim = 1)
                     )
model.to(device)

# Define the loss
criterion = nn.MSELoss()

# Define the optimizer (comment out one)
#optimizer = optim.Adam(model.parameters(), lr = 0.002)
optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9)

# Define the epochs train, test histories
epochs = 50
train_losses, test_losses = [], []
train_acc, test_acc = [], []

for e in range(epochs):
  running_loss = 0
  train_accuracy = 0

  for images, labels in trainloader:
    # Flatten MNIST images into a 784 long vector
    images = images.to(device)
    labels = labels.to(device)
    images = images.view(images.shape[0], -1)
    
    # Training pass
    optimizer.zero_grad()
    output = model.forward(images)
    loss = criterion(output, F.one_hot(labels,num_labels).float())
    loss.backward()
    optimizer.step()
    running_loss += loss.item()

    # check the accuracies
    top_p, top_class = output.topk(1, dim = 1)
    equals = top_class == labels.view(*top_class.shape)
    train_accuracy += torch.mean(equals.type(torch.FloatTensor))/len(trainloader)

  else:
    test_loss = 0
    test_accuracy = 0
    
    # Turn off gradients for validation, saves memory and computation
    with torch.no_grad():
      # Set the model to evaluation mode
      model.eval()
      
      # Test pass
      for images, labels in testloader:
        # Flatten MNIST images into a 784 long vector
        images = images.to(device)
        labels = labels.to(device)
        images = images.view(images.shape[0], -1)

        # prediction
        ps = model(images)
        test_loss += criterion(ps,  F.one_hot(labels,num_labels).float())
        
        # check the accuracies
        top_p, top_class = ps.topk(1, dim = 1)
        equals = top_class == labels.view(*top_class.shape)
        test_accuracy += torch.mean(equals.type(torch.FloatTensor))/len(testloader)
    
    # set model to training mode for next epoch
    model.train()
    train_losses.append(running_loss/len(trainloader))
    test_losses.append(test_loss/len(testloader))
    train_acc.append(train_accuracy)
    test_acc.append(test_accuracy)

    print("Epoch: {}/{}..".format(e+1, epochs),
          "Training loss: {:.3f}..".format(running_loss/len(trainloader)),
          "Test loss: {:.3f}..".format(test_loss/len(testloader)),
          "Train Accuracy: {:.3f}".format(train_accuracy),
          "Test Accuracy: {:.3f}".format(test_accuracy))
plt.plot(np.arange(len(train_acc)), train_acc)
plt.plot(np.arange(len(test_acc)), test_acc)

plt.rcParams["figure.dpi"] = 200
plt.plot(np.arange(len(train_acc)), train_acc,  label = 'Train Accuracy')
plt.plot(np.arange(len(test_acc)), test_acc, label = 'Test Accuracy')
plt.legend()
plt.title('Test & Training Accuracy v/s Epochs')
plt.xlabel("Epochs")
#plt.axes(xlabel = 'Epochs')

plt.rcParams["figure.dpi"] = 200
plt.plot(np.arange(len(train_losses)), train_losses,  label = 'Train Loss')
plt.plot(np.arange(len(test_losses)), test_acc, label = 'Test Loss')
plt.legend()
plt.title('Test & Training Loss v/s Epochs')
plt.xlabel("Epochs")
#plt.axes(xlabel = 'Epochs')